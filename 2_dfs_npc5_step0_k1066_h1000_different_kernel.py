# -*- coding: utf-8 -*-
"""DFS-NPC5-step0-k1066_h1000_different_kernel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gp7ozLfBrl0Hr0Ro3E80Qh30RYxczRNW
"""

# -*- coding: utf-8 -*-
"""DFS-NPC5-step0-k1066_h1000_different_kernel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gp7ozLfBrl0Hr0Ro3E80Qh30RYxczRNW
"""

# -*- coding: utf-8 -*-
"""modifed from: DFS-NPC4-forloop-k & collision_type-forEB-nospeed_lanes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GHR9sQx__DCBOUTqz3J751KFm_NyAG-y
"""

import pymc as pm
import numpy as np
import pandas as pd
import arviz as az
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from scipy.stats import norm
from scipy.spatial.distance import cdist
import os
import shutil
from shutil import copyfile
import gzip
import json
from tqdm import tqdm
import pickle


collision_types = ['overall','PDO','Severe']
kernel_functions = ['Triangular','Epanechnikov','Quartic']

h=1000
k=1066

output_folder = '/content/sample_data/5_modelling_results_nospeed_lanes-k1066_h1000_diffferent_kernel'
if not os.path.exists(output_folder):
    os.makedirs(output_folder)
else:
    try:
        for ea in os.listdir(output_folder):
            os.remove(output_folder + '/' + ea)
    except:
        for ea in os.listdir(output_folder):
            shutil.rmtree(output_folder + '/' + ea)

for collision_type in collision_types:

  dfs_file = '/content/drive/MyDrive/Postdoc McGill/reseach/DFS/starting from 20250723 proposed approach/4_prepared_DFS_sites_'+collision_type+'_road_props.csv'
  nondfs_file = '/content/drive/MyDrive/Postdoc McGill/reseach/DFS/starting from 20250723 proposed approach/4_prepared_nonDFS_sites_'+collision_type+'_road_props.csv'
  ##################

  ####FOR DFS sites
  df_dfs_segs = pd.read_csv(dfs_file)
  df = df_dfs_segs.copy()

  data_dfs = pd.DataFrame({
      'Index': df["Index"].values.tolist(),
      'length_m': df["Length"].values.tolist(),
      'years': df["before_years"].values.tolist(),
      'traffic_volume': df['before_avgADT'].values.tolist(),
      'observed_crashes': df['(10)Observed crash frequency in before period'].values.tolist(),
      'x_coord': df['Longitude'].values.tolist(),
      'y_coord': df['Latitude'].values.tolist(),
      #'speed_limit': df['speed_osm'].values.tolist(),
      'speed_limit': [1 if x > 40 else 0 for x in df['speed_osm'].values.tolist()],
      #'lanes': df['lanes_osm'].values.tolist(),
      'lanes': [1 if x > 2 else 0 for x in df['lanes_osm'].values.tolist()],
      'if_dfs': 0,
  })

  print('len(data_dfs):', len(data_dfs))

  for col in ['traffic_volume','observed_crashes']:
      data_dfs[col] = pd.to_numeric(data_dfs[col], errors='coerce')
  data_dfs = data_dfs.dropna()
  print('len(data_dfs):', len(data_dfs))

  ####FOR nonDFS sites
  df_nondfs_segs = pd.read_csv(nondfs_file)
  df = df_nondfs_segs.copy()

  data_nondfs = pd.DataFrame({
      'Index': df["Index"].values.tolist(),
      'length_m': df["Length"].values.tolist(),
      'years': 10,
      'traffic_volume': df['avgADT'].values.tolist(),
      'observed_crashes': df['SumCollisions'].values.tolist(),
      'x_coord': df['Longitude'].values.tolist(),
      'y_coord': df['Latitude'].values.tolist(),
      #'speed_limit': df['speed_osm'].values.tolist(),
      'speed_limit': [1 if x > 40 else 0 for x in df['speed_osm'].values.tolist()],
      #'lanes': df['lanes_osm'].values.tolist(),
      'lanes': [1 if x > 2 else 0 for x in df['lanes_osm'].values.tolist()],
      'if_dfs': 0,
  })

  print('len(data_nondfs):', len(data_nondfs))

  for col in ['traffic_volume','observed_crashes']:
      data_nondfs[col] = pd.to_numeric(data_nondfs[col], errors='coerce')
  data_nondfs = data_nondfs.dropna()
  print('len(data_dfs):', len(data_dfs))

  data = pd.concat([data_dfs, data_nondfs])
  print('len(data):', len(data))
  midpoints_ids = data["Index"].values.tolist()

  try:
    # Optional: Use a separate list of knots (e.g. from K-medoids)
    knots_df = pd.read_csv('/content/drive/MyDrive/Postdoc McGill/reseach/DFS/starting from 20250616/2_kmedoids_centroids_k'+str(k)+'_coords.csv')
    knots_ids = knots_df['medoid_id'].values.tolist()
    #knots_ids = data[data['if_dfs']==1]['Index'].values.tolist()
  except:
    knots_df = []

    # === Compute reweighted kernel matrix ===

    # Build set of used midpoint IDs from the large file
    used_ids = set()

    with gzip.open("/content/drive/MyDrive/Postdoc McGill/reseach/DFS/3_3_paths_with_legs_ALL_combined.jsonl.gz", "rt", encoding="utf-8") as f:
        for line in tqdm(f, desc="Scanning for knots"):
            record = json.loads(line)
            o_id = record["origin_id"]
            d_id = record["destination_id"]
            if o_id in midpoints_ids:
                used_ids.add(o_id)
            if d_id in midpoints_ids:
                used_ids.add(d_id)

    # Now define knots_ids
    knots_ids = sorted(list(used_ids))


  filename = output_folder+'/5_knots_ids_k'+str(k)+'.pkl'
  with open(filename, 'wb') as f:
      pickle.dump(knots_ids, f)


  print(f"âœ… Found {len(knots_ids)} knots out of {len(midpoints_ids)} midpoints.")


  def compute_reweighted_kernels(midpoints_ids, knots_ids, kernel_function='Triangular', path_to_jsonl="/content/drive/MyDrive/Postdoc McGill/reseach/DFS/3_3_paths_with_legs_ALL_combined.jsonl.gz", sp_lookup_path=None):
      """Compute reweighted kernel matrix from streamed JSONL input."""

      K_weighteds = np.zeros((len(midpoints_ids), len(knots_ids)))

      # For fast lookup
      midpoint_set = set(midpoints_ids)
      knot_set = set(knots_ids)
      if sp_lookup_path == None:
        # Build lookup dictionary of relevant shortest paths
        sp_lookup = {}

        print("ðŸ“¦ Indexing valid shortest paths...")
        with gzip.open(path_to_jsonl, "rt", encoding="utf-8") as f:
            for line in tqdm(f, desc="Indexing JSONL lines"):
                sp_info = json.loads(line)
                i, j = sp_info["origin_id"], sp_info["destination_id"]

                # Only keep if (midpoint â†” knot) pair
                if (i in midpoint_set and j in knot_set) or (j in midpoint_set and i in knot_set):
                    key = tuple(sorted((i, j)))
                    sp_lookup[key] = sp_info

        print(f"âœ… Indexed {len(sp_lookup)} valid pairs.")

      else:
        sp_lookup = pickle.load(open(sp_lookup_path, "rb"))
        print(f"âœ… Loaded {len(sp_lookup)} valid pairs from {sp_lookup_path}.")

      # Compute kernel matrix
      print("ðŸ§® Computing reweighted kernel matrix...")
      for i_idx, midpoint_id in enumerate(tqdm(midpoints_ids, desc="Rows")):
          for j_idx, knot_id in enumerate(knots_ids):
              if midpoint_id == knot_id:
                  sp = 0
                  weights = [1]
              else:
                  key = tuple(sorted((midpoint_id, knot_id)))
                  sp_info = sp_lookup.get(key)

                  if not sp_info:
                      sp = np.inf
                      weights = [1]
                  else:
                      sp = sp_info.get("distance", 0)
                      weights = []
                      total_legs = sp_info.get("total_legs", [])
                      ac_legs = sp_info.get("ac_legs", [])

                      for total, ac in zip(total_legs, ac_legs):
                          if ac > 2:
                              w = (2 / ac) * (1 / (ac - 1))
                          else:
                              w = 1
                          weights.append(w)

                      if not weights:
                          weights = [1]
              if np.isinf(sp):
                K = 0  # no connection
              else:
                reweight = np.prod(weights)
                if kernel_function == 'Triangular':
                    K = (reweight ** 0.5) * max(1 - abs(sp / h), 0)
                elif kernel_function == 'Epanechnikov':
                    K = (reweight ** 0.5) * max((3/4)*(1-(sp/h)**2),0)
                elif kernel_function == 'Quartic':
                    if sp/h <1:
                      K = (reweight ** 0.5) * (15/16)*((1-(sp/h)**2)**2)
                    else:
                      K= 0
              K_weighteds[i_idx, j_idx] = K

      return K_weighteds

  for kernel_function in kernel_functions:
    try:
      filename = output_folder+f'/5_K_weighted_k{k}_h{h}_{kernel_function}.pkl'
      with open(filename, 'rb') as f:
        K_weighted = pickle.load(f)
      print("âœ… Kernel matrix shape:", K_weighted.shape)
    except:
      # After defining midpoints_ids and knots_ids:
      print('Here, using kernel:', kernel_function)
      K_weighted = compute_reweighted_kernels(midpoints_ids, knots_ids, kernel_function=kernel_function, sp_lookup_path='/content/drive/MyDrive/Postdoc McGill/reseach/DFS/starting from 20250623/_sp_lookups.pkl')
      print("âœ… Kernel matrix shape:", K_weighted.shape)

      filename = output_folder+f'/5_K_weighted_k{k}_h{h}_{kernel_function}.pkl'
      with open(filename, 'wb') as f:
        pickle.dump(K_weighted, f)
      print("âœ… Kernel matrix shape:", K_weighted.shape)

    # ==============================================
    # Step 3: Fit NPC Model with Reweighted Kernel
    # ==============================================
    # === Prepare Covariate Matrix ===
    # Standardize log(traffic volume)
    traffic_volume_log = np.log(data['traffic_volume'].values)
    traffic_volume_log = (traffic_volume_log - traffic_volume_log.mean()) / traffic_volume_log.std()
    data['log_traffic_volume_std'] = traffic_volume_log

    '''
    speed_limits = data['speed_limit'].values
    speed_limits = (speed_limits - speed_limits.mean()) / speed_limits.std()
    data['speed_limit_std'] = speed_limits

    lanes = data['lanes'].values
    lanes = (lanes - lanes.mean()) / lanes.std()
    data['lanes_std'] = lanes
    '''

    # Build X matrix with continuous and dummy covariates
    X = data[['log_traffic_volume_std']].values

    with pm.Model() as model:
      # Priors
      beta0 = pm.Normal("beta0", mu=0, sigma=1)  # Tightened priors
      #beta = pm.Normal("beta", mu=0, sigma=1, shape=X.shape[1])
      var_psi = pm.Gamma("var_psi", alpha=2, beta=2)  # variance
      sigma_psi = pm.Deterministic("sigma_psi", pm.math.sqrt(var_psi))
      psi = pm.Normal("psi", mu=0, sigma=sigma_psi, shape=len(knots_ids))

      # Spatial random effects with reweighted kernel
      Z = pm.math.dot(K_weighted, psi)

      # Expected crashes (log-link with offset)
      #log_lambda = pm.math.log(data['length_m'].values) + pm.math.log(data['years'].values) + beta0 + pm.math.dot(X, beta) + Z
      log_lambda = pm.math.log(data['length_m'].values) + pm.math.log(data['years'].values) + beta0 + Z
      lambda_ = pm.Deterministic("lambda_", pm.math.exp(log_lambda))

      # Likelihood
      y_obs = pm.Poisson("y_obs", mu=lambda_, observed=data['observed_crashes'].values)

      # Sample
      trace = pm.sample(draws=3000,
                        tune=1000,
                        chains=2,
                        target_accept=0.95,         # More cautious steps
                        max_treedepth=15,
                        random_state=42,
                        return_inferencedata=True,
                        idata_kwargs={"log_likelihood": True})


    # Save trace
    az.to_netcdf(trace, output_folder+f"/5_trace_k{k}_h{h}_{collision_type}_{kernel_function}.nc")

    idata = az.from_netcdf(output_folder+f"/5_trace_k{k}_h{h}_{collision_type}_{kernel_function}.nc")
    # WAIC
    waic = az.waic(idata)
    print("WAIC:", waic)

    # Save to file
    with open(output_folder+f"/5_waic_k{k}_h{h}_{collision_type}_{kernel_function}.pkl", "wb") as f:
        pickle.dump(waic, f)

    # LOO
    loo = az.loo(idata)
    print("LOO:", loo)

    # Save to file
    with open(output_folder+f"/5_loo_k{k}_h{h}_{collision_type}_{kernel_function}.pkl", "wb") as f:
        pickle.dump(loo, f)


    # Summarize the posterior
    # Posterior Summary
    #summary = az.summary(trace, var_names=["beta0", "beta", "sigma_psi"], round_to=4, hdi_prob=0.95)#
    summary = az.summary(trace, var_names=["beta0", "var_psi"], round_to=4, hdi_prob=0.95)

    # Label beta coefficients dynamically
    #beta_labels = [ "DFS", "Lane Numbers"] + dummy_cols
    #beta_labels = ["Traffic Volume"]#
    #summary.index = ["Intercept (beta0)"] + [f"Beta[{i}] - {label}" for i, label in enumerate(beta_labels)] + ["Sigma_psi"]#

    # Output stats
    summary_stats = summary[["mean", "sd", "hdi_2.5%", "hdi_97.5%", "ess_bulk", "r_hat"]].copy()
    summary_stats.to_csv(output_folder+f'/5_summary_stats_k{k}_h{h}_{collision_type}_{kernel_function}.csv')
    print(summary_stats)

    '''## Trace and Posterior Plots (Beta Coefficients, All)
    # Trace and Posterior for all beta coefficients
    n_betas = X.shape[1]
    n_rows = n_betas
    plt.figure(figsize=(14, 5 * n_rows))

    for i in range(n_betas):
        # Trace
        plt.subplot(n_rows, 2, 2 * i + 1)
        for chain in trace.posterior.chain.values:
            plt.plot(trace.posterior.beta.sel(chain=chain, beta_dim_0=i),
                    alpha=0.5, label=f'Chain {chain+1}')
        plt.title(f'{beta_labels[i]} - Trace')
        plt.xlabel('Iteration')
        plt.ylabel('Value')
        if i == 0:
            plt.legend()

        # Posterior
        plt.subplot(n_rows, 2, 2 * i + 2)
        az.plot_posterior(trace.posterior.beta.sel(beta_dim_0=i), ax=plt.gca(), hdi_prob=0.95)
        plt.title(f'{beta_labels[i]} - Posterior')
        plt.xlabel('Log Effect')

    plt.tight_layout()
    plt.savefig(output_folder+"/5_trace_k"+str(k)+"_"+str(k)+'_'+collision_type+'.jpg', dpi=300)
    #plt.show()
    plt.close('all')'''

    '''# ==============================================
    # Step 6: Visualize Spatial Effects (psi) on Map
    # ==============================================
    import matplotlib.pyplot as plt
    !pip install contextily
    import contextily as ctx
    import pandas as pd
    import numpy as np

    # Deduplicate data by "Index" so each knot_id has one coordinate
    data_unique = data.drop_duplicates(subset="Index").set_index("Index")

    # Ensure knot ordering matches psi
    psi_post = trace.posterior["psi"].mean(dim=("chain", "draw")).values  # shape: (2335,)
    assert len(psi_post) == len(knots_ids), "Mismatch between psi length and knots_ids"

    # Filter and align coordinates
    # comment the below for reading other files for the knots
    if knots_df is []:
      knots_df = data_unique.loc[knots_ids].reset_index()
    knots_df["psi"] = psi_post

    # === Plotting ===
    fig, ax = plt.subplots(figsize=(12, 10))

    try:
      scatter = ax.scatter(
          knots_df["x_coord"],
          knots_df["y_coord"],
          c=knots_df["psi"],
          s=100,
          cmap="coolwarm",
          alpha=0.8,
          edgecolor="k",
          linewidth=0.5
      )
    except:
      scatter = ax.scatter(
          knots_df["Longitude"],
          knots_df["Latitude"],
          c=knots_df["psi"],
          s=100,
          cmap="coolwarm",
          alpha=0.8,
          edgecolor="k",
          linewidth=0.5
      )

    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label("Spatial Effect (psi)", fontsize=12)

    # Optional annotation (disabled by default)
    # for i, row in knots_df.iterrows():
    #     ax.annotate(str(row["Index"]), (row["Proj Long"] + 50, row["Proj Lat"] + 50), fontsize=8, color="black")

    # Add basemap
    try:
        ctx.add_basemap(
            ax,
            crs="EPSG:4326",  # keep this
            source=ctx.providers.CartoDB.Positron,
            zoom=12
        )
    except Exception as e:
        print("Basemap failed:", e)

    ax.set_title("Spatial Random Effects (psi) at Knot Locations", fontsize=14)
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # Save and show
    plt.savefig(output_folder+"/5_psi_k"+str(k)+'_'+collision_type+'.jpg', dpi=300, bbox_inches="tight")
    #plt.show()
    plt.close('all')'''

from google.colab import files
import os

# Define the directory to be zipped and the desired output filename
dir_to_zip = output_folder  # Replace with the actual folder name in Colab
output_filename = output_folder.split('/')[-1]+'.zip'

# Create a dummy folder and file for demonstration (optional)
# if not os.path.exists(dir_to_zip):
#     os.makedirs(dir_to_zip)
# with open(os.path.join(dir_to_zip, 'example.txt'), 'w') as f:
#     f.write('This is an example file.')

# Zip the folder
# The -r flag includes subdirectories, and the first argument is the output zip file
os.system(f"zip -r {output_filename} {dir_to_zip}")

# Download the zipped file
files.download(output_filename)

# Optional: Delete the zipped file from Colab after download
# os.remove(output_filename)